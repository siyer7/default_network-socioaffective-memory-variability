{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42719274",
   "metadata": {},
   "source": [
    "# 1. Establish memory sentiment using sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3696486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "# set as: \n",
    "# 'baseline' for baseline rest\n",
    "# 'soc' for video encoding, and\n",
    "# 'rest' for consolidation\n",
    "stim = 'soc'\n",
    "exclude_subs = [1,6,14,18,21,31]\n",
    "\n",
    "\n",
    "# CHANGE TO DIRECTORY CONTAINING PREPROCESSED FILES\n",
    "src = '/Users/f0064z8/Dropbox (Dartmouth College)/DSNL Team Folder/EmpOrient_fMRI/Data/'\n",
    "\n",
    "os.chdir('.')\n",
    "\n",
    "# get participant recall data\n",
    "txt_df = pd.read_csv(src+'processed/post_task/social_memory_freewrites.csv')\n",
    "txt_df = txt_df[~txt_df['subject'].isin(exclude_subs)].reset_index(drop=True)\n",
    "txt_df = txt_df.sort_values(by=['subject']).reset_index(drop=True)\n",
    "\n",
    "# COLLATING TEXT ACROSS THE 4 PATIENT VIDEOS\n",
    "subs_txt=[]\n",
    "for sub in range(len(txt_df)):    \n",
    "\n",
    "    sub_txt=''\n",
    "    for patient in range (2,6): # looping through cols of the 4 patient videos\n",
    "        sub_txt = sub_txt + txt_df[txt_df.columns[patient]].iloc[sub] + '\\n'\n",
    "    subs_txt.append(sub_txt)\n",
    "\n",
    "txt_df['soc_txt']=subs_txt\n",
    "txt_df = txt_df[['subject','soc_txt']]\n",
    "\n",
    "print(txt_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08021957",
   "metadata": {},
   "source": [
    "# Combine text data across patients in a new df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92dff8f2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    subject                                            soc_txt\n",
      "0         2  When you are young, there are some signs that ...\n",
      "1         3  The diagnosis of CF, with its symptoms. \\nCF g...\n",
      "2         4  CF is a genetic disease, and babies are tested...\n",
      "3         5  If there is a genetic trend of CF in your fami...\n",
      "4         7  The sweat test checks Cl in the sweat to see i...\n",
      "5         8  I remember that it has a lot to do with genes....\n",
      "6        10  CF is a genetic disease that can only be contr...\n",
      "7        11  This video was about the clues that show signs...\n",
      "8        12  I remember the speaker talking about what test...\n",
      "9        13  You can diagnose a baby through a sweat test w...\n",
      "10       15  Babies are given a newborn screen (if they are...\n",
      "11       16  This video was about Cystic Fibrosis Diagnosis...\n",
      "12       19  I did not recall much from this video, at this...\n",
      "13       20  THis video talked about how doctors test for C...\n",
      "14       22  Oh man, not very much. I know in this video sh...\n",
      "15       25  When babies are first born they are tested and...\n",
      "16       26  i may have dozed off during this one some.   i...\n",
      "17       27  The speaker talked about what is involved in d...\n",
      "18       30  BABIES CAN BE TESTED THROUGH THEIR SWEAT, AS I...\n",
      "19       32  This video talked about the type of testing th...\n",
      "20       34  The video that relates to this slide talked ab...\n",
      "21       36  I was unable to hear some of this video, howev...\n",
      "22       37  Diagnosis is made by looking at sweat test res...\n",
      "23       38  Diagnosis is often given in infancy. When a ba...\n",
      "24       40  They test newborns for CF.\\nSweat is a good me...\n",
      "25       41  This video talked about how CF is diagnosed. I...\n",
      "26       42  babies should pass meconium within a few hours...\n",
      "27       44  Newborns would only be given this test if ther...\n",
      "28       45  This video was talking about ways to know if s...\n",
      "29       46  Overall, I thought the video was well done. Th...\n",
      "30       47  Very interesting, fascinating really.  Enjoy l...\n",
      "31       48  That there has to be two little b to have cf. ...\n",
      "32       49  The gold standard for CF diagnosis is the swea...\n",
      "33       51  There are different ways to diagnose CF. One i...\n",
      "34       52  Newborn babies are diagnosed of cystic fibrosi...\n",
      "35       54  One of the ways to test for CF is the sweat te...\n",
      "36       55  I remember how the passageway between the pore...\n",
      "37       56  Medical examiners conduct a sweat test in orde...\n",
      "38       57  CF can be diagnosed in two major ways. The fir...\n",
      "39       58  They do sweat test on babies to see if they ha...\n"
     ]
    }
   ],
   "source": [
    "stim = 'soc'\n",
    "exclude_subs = [1,6,14,18,21,31]\n",
    "src = '/Users/f0064z8/Dropbox (Dartmouth College)/DSNL Team Folder/EmpOrient_fMRI/Data/'\n",
    "os.chdir('/Users/f0064z8/Dropbox (Dartmouth College)/DSNL Team Folder/Sid/behavioral/')\n",
    "\n",
    "# get participant recall data\n",
    "txt_df = pd.read_csv(src+'processed/post_task/social_memory_freewrites.csv')\n",
    "txt_df = txt_df[~txt_df['subject'].isin(exclude_subs)].reset_index(drop=True)\n",
    "txt_df = txt_df.sort_values(by=['subject']).reset_index(drop=True)\n",
    "\n",
    "# COLLATING TEXT ACROSS THE 4 PATIENT VIDEOS\n",
    "subs_txt=[]\n",
    "for sub in range(len(txt_df)):    \n",
    "\n",
    "    sub_txt=''\n",
    "    for patient in range (2,6): # looping through cols of the 4 patient videos\n",
    "        sub_txt = sub_txt + txt_df[txt_df.columns[patient]].iloc[sub] + '\\n'\n",
    "    subs_txt.append(sub_txt)\n",
    "\n",
    "txt_df['soc_txt']=subs_txt\n",
    "txt_df = txt_df[['subject','soc_txt']]\n",
    "\n",
    "print(txt_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ea60ae",
   "metadata": {},
   "source": [
    "# Clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4734255c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = txt_df.copy()\n",
    "# removing everything except alphabets\n",
    "clean_df['soc_txt'] = clean_df['soc_txt'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "# removing short words\n",
    "clean_df['soc_txt'] = clean_df['soc_txt'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
    "# make all text lowercase\n",
    "clean_df['soc_txt'] = clean_df['soc_txt'].apply(lambda x: x.lower())\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# tokenization\n",
    "tokens = clean_df['soc_txt'].apply(lambda x: x.split())\n",
    "# remove stop-words\n",
    "tokens = tokens.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "# de-tokenization\n",
    "detokenized = []\n",
    "for sub in range(len(clean_df)):\n",
    "    token = ' '.join(tokens[sub])\n",
    "    detokenized.append(token)\n",
    "    \n",
    "clean_df['soc_txt'] = detokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cd12a2",
   "metadata": {},
   "source": [
    "# Conduct sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "476993fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['like:0.3612', 'well:0.2732', 'like:0.3612']\n",
      "['failure:-0.5106', 'trouble:-0.4019', 'hard:-0.1027']\n",
      "['']\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a9f397a79889>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msub\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoc_sent_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0msoc_sent_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'pos_words'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoc_sent_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pos_words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0msoc_sent_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'neg_words'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoc_sent_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'neg_words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-a9f397a79889>\u001b[0m in \u001b[0;36mtop_words\u001b[0;34m(words)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mdict_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m':'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m':'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mdict_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sentiment(txt):\n",
    "    tokens = nltk.word_tokenize(txt)\n",
    "    score = 0\n",
    "    abs_score = 0\n",
    "    length = len(tokens)\n",
    "    n_val = 0\n",
    "    \n",
    "    pos_words=[]\n",
    "    neg_words=[]\n",
    "    \n",
    "    for token in tokens:\n",
    "        \n",
    "        token_score = sia.polarity_scores(token)['compound']        \n",
    "        score = score + token_score\n",
    "        abs_score = abs_score + np.abs(token_score)\n",
    "        \n",
    "        if token_score > 0:\n",
    "            pos_words.append(f'{token}:{token_score}')\n",
    "            n_val = n_val + 1\n",
    "            \n",
    "        elif token_score < 0:\n",
    "            neg_words.append(f'{token}:{token_score}')\n",
    "            n_val = n_val + 1\n",
    "            \n",
    "    return pos_words, neg_words, score, abs_score, np.float64(n_val), np.float64(length) \n",
    "\n",
    "sub_sent = pd.DataFrame(columns = ['pos_words','neg_words','sentiment_score','abs_score','n_val','length'])\n",
    "\n",
    "for sub in range(len(clean_df)):\n",
    "    pos_words, neg_words, score, abs_score, n_val, length = sentiment(clean_df['soc_txt'].iloc[sub])\n",
    "    sub_sent = sub_sent.append(pd.DataFrame({\"pos_words\":[pos_words],\\\n",
    "                                             \"neg_words\":[neg_words],\\\n",
    "                                             \"sentiment_score\": [score],\\\n",
    "                                             \"abs_score\": [abs_score],\\\n",
    "                                             \"n_val\": [n_val],\\\n",
    "                                             \"length\": [length]},\\\n",
    "                                              index=[sub]))\n",
    "\n",
    "soc_sent_df = txt_df.copy()\n",
    "soc_sent_df['pos_words'] = sub_sent['pos_words']\n",
    "soc_sent_df['neg_words'] = sub_sent['neg_words']\n",
    "soc_sent_df['sentiment_score'] = sub_sent['sentiment_score']\n",
    "soc_sent_df['abs_score'] = sub_sent['abs_score']\n",
    "soc_sent_df['n_val'] = sub_sent['n_val']\n",
    "soc_sent_df['length'] = sub_sent['length']\n",
    "soc_sent_df.to_csv(f'./{stim}_sent_all_txt.csv', index=False)\n",
    "\n",
    "soc_sent_df = pd.read_csv(f'./{stim}_sent_all_txt.csv')\n",
    "soc_sent_df = soc_sent_df[~soc_sent_df['subject'].isin(exclude_subs)].reset_index(drop=True)\n",
    "\n",
    "def top_words(words):\n",
    "    \n",
    "    words = [w for w in words if w.isalnum() or w in [',', '.', '-', ':', '(', ')']]\n",
    "    words = \"\".join(words)\n",
    "    words = list(words.split(\",\"))\n",
    "\n",
    "    dict_words = {}\n",
    "    \n",
    "    print(words)\n",
    "    \n",
    "    for word in words:\n",
    "        dict_words[np.abs(float(word.split(':')[1]))] = word.split(':')[0].strip()\n",
    "    \n",
    "    dict_words = dict(sorted(dict_words.items()))\n",
    "    \n",
    "    return list(dict_words.values())\n",
    "\n",
    "for sub in range(len(soc_sent_df)):\n",
    "    soc_sent_df.at[sub,'pos_words'] = list(reversed(top_words(soc_sent_df['pos_words'].loc[sub])))\n",
    "    soc_sent_df.at[sub,'neg_words'] = top_words(soc_sent_df['neg_words'].loc[sub])\n",
    "\n",
    "    \n",
    "soc_sent_df[['pos_words','neg_words','sentiment_score']].head()\n",
    "soc_sent_df.to_csv(f'./{stim}_sent_all_txt.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6b220a",
   "metadata": {},
   "source": [
    "# Merge all behavioral data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4288f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def filter_subs(df):\n",
    "#     df = df[~df['subject'].isin(exclude_subs)].reset_index(drop=True)\n",
    "#     df = df.sort_values(by=['subject']).reset_index(drop=True)\n",
    "#     return df\n",
    "\n",
    "# state = pd.read_csv(src+'processed/task/empathy_data.csv')\n",
    "# state = state.rename(columns={\"socialVid_emp\":\"state_emp\", \"group\":\"order\"})\n",
    "# state = filter_subs(state)\n",
    "# state = state[['subject','state_emp','order']]\n",
    "\n",
    "# loneliness = pd.read_csv(src+'processed/trait/loneliness_data.csv')\n",
    "# loneliness = loneliness.rename(columns={\"loneliness_score\":\"loneliness\"})\n",
    "# loneliness = filter_subs(loneliness)\n",
    "# loneliness = loneliness[['subject',\"loneliness\"]]\n",
    "\n",
    "# sent = pd.read_csv('./soc_sent.csv')\n",
    "# sent = filter_subs(sent)\n",
    "\n",
    "# facts = pd.read_csv(src+'raw/post_task/facts.csv')\n",
    "# facts = filter_subs(facts)\n",
    "# facts = facts[['subject','facts']]\n",
    "\n",
    "# merged_df = pd.concat([state,loneliness,sent,facts], axis=1)\n",
    "# merged_df.to_csv('./behavioral_data_tmp.csv', index=False)\n",
    "# df_corr = merged_df.corr().round(2)\n",
    "# df_corr.insert(0, 'cols', df_corr.columns)\n",
    "# df_corr.to_csv('./behavioral_data_corr_tmp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d31ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
