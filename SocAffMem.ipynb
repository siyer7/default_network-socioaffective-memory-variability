{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02f4b29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os, glob, random, copy\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import rankdata, ttest_rel, ttest_1samp, zscore\n",
    "from numpy import linalg as LA\n",
    "\n",
    "from matplotlib import colors\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.transforms as mtransforms\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import nibabel as nib\n",
    "from nilearn.input_data import NiftiLabelsMasker\n",
    "from nilearn import plotting\n",
    "\n",
    "from nltools.data import Brain_Data, Adjacency\n",
    "from nltools.mask import roi_to_brain, expand_mask\n",
    "from nltools.stats import fdr, threshold, _calc_pvalue\n",
    "from nltools.utils import get_anatomical\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "import datalad.api as dl\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42719274",
   "metadata": {},
   "source": [
    "# 1. Establish memory sentiment using sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3696486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set as: \n",
    "# 'baseline' for baseline rest\n",
    "# 'soc' for video encoding, and\n",
    "# 'rest' for consolidation\n",
    "stim = 'soc'\n",
    "exclude_subs = [1,6,14,18,21,31]\n",
    "\n",
    "\n",
    "# get participant recall data\n",
    "txt_df = pd.read_csv('./social_memory_freewrites.csv')\n",
    "# exclude subs\n",
    "txt_df = txt_df[~txt_df['subject'].isin(exclude_subs)].reset_index(drop=True)\n",
    "# sort by sub\n",
    "txt_df = txt_df.sort_values(by=['subject']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# COMBINING TEXT ACROSS THE 4 (social or science, depending on 'stim' variable) VIDEOS\n",
    "\n",
    "subs_txt = []\n",
    "for sub in range(len(txt_df)):    \n",
    "\n",
    "    sub_txt=''\n",
    "    for patient in range (2,6): # looping through columns of the 4 videos\n",
    "        sub_txt = sub_txt + txt_df[txt_df.columns[patient]].iloc[sub] + '\\n'\n",
    "    subs_txt.append(sub_txt)\n",
    "\n",
    "# append column which has recall text for all 4 videos\n",
    "txt_df['soc_txt']=subs_txt\n",
    "# only retain subID and combined text columns\n",
    "txt_df = txt_df[['subject','soc_txt']]\n",
    "\n",
    "# print(txt_df)\n",
    "\n",
    "\n",
    "# CLEAN TEXT\n",
    "\n",
    "clean_df = txt_df.copy()\n",
    "# removing everything except alphabets\n",
    "clean_df['soc_txt'] = clean_df['soc_txt'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "# removing short words\n",
    "clean_df['soc_txt'] = clean_df['soc_txt'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
    "# make all text lowercase\n",
    "clean_df['soc_txt'] = clean_df['soc_txt'].apply(lambda x: x.lower())\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# tokenization: store each word of corpus as item in list\n",
    "tokens = clean_df['soc_txt'].apply(lambda x: x.split())\n",
    "# remove stop-word items\n",
    "tokens = tokens.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "# de-tokenization: recombine list of stop-word excluded words into string corpus\n",
    "detokenized = []\n",
    "for sub in range(len(clean_df)):\n",
    "    token = ' '.join(tokens[sub])\n",
    "    detokenized.append(token)\n",
    "    \n",
    "clean_df['soc_txt'] = detokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cd12a2",
   "metadata": {},
   "source": [
    "# Conduct sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "476993fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(txt):\n",
    "    \n",
    "    # create list of tokens (words) from string\n",
    "    tokens = nltk.word_tokenize(txt)\n",
    "    length = len(tokens)\n",
    "    \n",
    "    # 'score' is our main variable of interest, ignore all other variables in code below\n",
    "    score = abs_score = n_val = 0\n",
    "    # list of pos & neg words, stored alongwith their scores, for e.g.:\n",
    "    # ['angry: -.437', 'delighted: .619'...]\n",
    "    pos_words = neg_words = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        \n",
    "        # get sentiment score of each word (token)\n",
    "        token_score = sia.polarity_scores(token)['compound']        \n",
    "        score = score + token_score\n",
    "        abs_score = abs_score + np.abs(token_score)\n",
    "        \n",
    "        if token_score > 0:\n",
    "            pos_words.append(f'{token}:{token_score}')\n",
    "            n_val = n_val + 1\n",
    "            \n",
    "        elif token_score < 0:\n",
    "            neg_words.append(f'{token}:{token_score}')\n",
    "            n_val = n_val + 1\n",
    "            \n",
    "    return pos_words, neg_words, score, abs_score, np.float64(n_val), np.float64(length) \n",
    "\n",
    "sub_sent = pd.DataFrame(columns = ['pos_words','neg_words','sentiment_score','abs_score','n_val','length'])\n",
    "\n",
    "for sub in range(len(clean_df)):\n",
    "    pos_words, neg_words, score, abs_score, n_val, length = sentiment(clean_df['soc_txt'].iloc[sub])\n",
    "    sub_sent = sub_sent.append(pd.DataFrame({\"pos_words\":[pos_words],\\\n",
    "                                             \"neg_words\":[neg_words],\\\n",
    "                                             \"sentiment_score\": [score],\\\n",
    "                                             \"abs_score\": [abs_score],\\\n",
    "                                             \"n_val\": [n_val],\\\n",
    "                                             \"length\": [length]},\\\n",
    "                                              index=[sub]))\n",
    "\n",
    "# store scores in df\n",
    "soc_sent_df = txt_df.copy()\n",
    "soc_sent_df['pos_words'] = sub_sent['pos_words']\n",
    "soc_sent_df['neg_words'] = sub_sent['neg_words']\n",
    "soc_sent_df['sentiment_score'] = sub_sent['sentiment_score']\n",
    "soc_sent_df['abs_score'] = sub_sent['abs_score']\n",
    "soc_sent_df['n_val'] = sub_sent['n_val']\n",
    "soc_sent_df['length'] = sub_sent['length']\n",
    "\n",
    "# store scores in df\n",
    "# the only columns relevant to our interests are:\n",
    "# 'sentiment_score', 'pos_words', 'neg_words'\n",
    "soc_sent_df.to_csv(f'./{stim}_sent_all_txt.csv', index=False)\n",
    "\n",
    "\n",
    "soc_sent_df = pd.read_csv(f'./{stim}_sent_all_txt.csv')\n",
    "# exclude subs\n",
    "soc_sent_df = soc_sent_df[~soc_sent_df['subject'].isin(exclude_subs)].reset_index(drop=True)\n",
    "\n",
    "# a way to store words in df columns in a sorted order based on sentiment scores, i.e.,\n",
    "# the 'pos_words' column has the most +ve words listed first, and\n",
    "# the 'neg_words' column has the most -ve words listed first\n",
    "def top_words(words):\n",
    "    \n",
    "    # remove non-words\n",
    "    words = [w for w in words if w.isalnum() or w in [',', '.', '-', ':', '(', ')']]\n",
    "    # convert list of words to string\n",
    "    words = \"\".join(words)\n",
    "    # convert string to list (not sure why I'm doing this but I don't want to change the code now)\n",
    "    words = list(words.split(\",\"))\n",
    "\n",
    "    dict_words = {}\n",
    "    \n",
    "#     print(words)\n",
    "    \n",
    "    for word in words:\n",
    "        # convert list of word-score combos to dict, such that \n",
    "        # keys are sentiment scores, and values are the words\n",
    "        # e.g. converting ['angry: -.437', 'delighted: .619'...] to { -.437: 'angry', .619: 'delighted', ...}\n",
    "        dict_words[np.abs(float(word.split(':')[1]))] = word.split(':')[0].strip()\n",
    "    \n",
    "    # sort dict by score\n",
    "    dict_words = dict(sorted(dict_words.items()))\n",
    "    \n",
    "    return list(dict_words.values())\n",
    "\n",
    "for sub in range(len(soc_sent_df)):\n",
    "    soc_sent_df.at[sub,'pos_words'] = list(reversed(top_words(soc_sent_df['pos_words'].loc[sub])))\n",
    "    soc_sent_df.at[sub,'neg_words'] = top_words(soc_sent_df['neg_words'].loc[sub])\n",
    "\n",
    "    \n",
    "soc_sent_df[['pos_words','neg_words','sentiment_score']].head()\n",
    "soc_sent_df.to_csv(f'./{stim}_sent_all_txt.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5a9587",
   "metadata": {},
   "source": [
    "# Convert preprocessed-to-csv fMRI data to (subject x TR x ROI) numpy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7ab1366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_subs, n_timepoints, n_parcels\n",
      "(40, 360, 50)\n",
      "n_subs, n_timepoints, n_parcels\n",
      "(40, 360, 50)\n",
      "n_subs, n_timepoints, n_parcels\n",
      "(40, 360, 50)\n",
      "n_subs, n_timepoints, n_parcels\n",
      "(40, 1023, 50)\n",
      "n_subs, n_timepoints, n_parcels\n",
      "(40, 1023, 50)\n"
     ]
    }
   ],
   "source": [
    "for phase in ['baseline', 'soc_consol', 'sci_consol']:\n",
    "    \n",
    "    # grab all files\n",
    "    files = glob.glob(f'./brain_data/*/*{phase}.csv')\n",
    "    files.sort()\n",
    "    data = []\n",
    "\n",
    "    for file in files:\n",
    "        data.append(pd.read_csv(file))\n",
    "\n",
    "    data = np.array(data)\n",
    "    print('n_subs, n_timepoints, n_parcels')\n",
    "    print(data.shape)\n",
    "    # save as subject by TR by ROI numpy file  \n",
    "    np.save(f'./brain_data/{phase}_nodewise_timeseries', data)\n",
    "\n",
    "\n",
    "for phase in ['soc_enc', 'sci_enc']:\n",
    "    \n",
    "    files = glob.glob(f'./brain_data/*/*{phase}*.csv')\n",
    "    files.sort()\n",
    "    data = []\n",
    "    sub_data = []\n",
    "    \n",
    "    for i in range(len(files)):\n",
    "        \n",
    "        f = files[i]\n",
    "        ts = pd.read_csv(f)              \n",
    "\n",
    "        # trim start and end fixation based on video\n",
    "        if 'jacob' in f:\n",
    "            ts = ts.iloc[14:len(ts)-12].reset_index(drop=True)\n",
    "        elif 'laura' in f:\n",
    "            ts = ts.iloc[16:len(ts)-14].reset_index(drop=True)\n",
    "        elif 'molly' in f:                \n",
    "            ts = ts.iloc[16:len(ts)-21].reset_index(drop=True)\n",
    "        elif 'morgan' in f:\n",
    "            ts = ts.iloc[12:len(ts)-12].reset_index(drop=True)\n",
    "        elif 'path' in f:\n",
    "            ts = ts.iloc[16:len(ts)-14].reset_index(drop=True)\n",
    "        elif 'symp' in f:\n",
    "            ts = ts.iloc[18:len(ts)-9].reset_index(drop=True)\n",
    "        elif 'diag' in f:                \n",
    "            ts = ts.iloc[14:len(ts)-18].reset_index(drop=True)\n",
    "        elif 'gene' in f:\n",
    "            ts = ts.iloc[12:len(ts)-16].reset_index(drop=True)\n",
    "        \n",
    "        # append these 4 (social or science) videos to each subject's data\n",
    "        sub_data.append(ts)\n",
    "        \n",
    "        # if we have traversed through all 4 videos per subject,\n",
    "        # append this subject's data to the list of all subjects' data, and\n",
    "        # flush out sub_data variable\n",
    "        if (i+1)%4 == 0:\n",
    "            sub_data = pd.concat(sub_data)\n",
    "            data.append(sub_data.values)\n",
    "            sub_data = []  \n",
    "\n",
    "    data = np.array(data)\n",
    "    print('n_subs, n_timepoints, n_parcels')\n",
    "    print(data.shape)\n",
    "    # save as subject by TR by ROI numpy file  \n",
    "    np.save(f'./brain_data/{phase}_nodewise_timeseries', data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8e1590",
   "metadata": {},
   "source": [
    "# Set up functions & variables for IS-RSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b9c2474",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cmap = plt.cm.get_cmap('RdYlBu_r')\n",
    "cmap.set_bad('#C0C0C0')\n",
    "\n",
    "def sort_square_mtx(mtx, vct):\n",
    "    \"\"\"\n",
    "    Sorts rows/columns of a matrix according to a separate vector.\n",
    "    \"\"\"\n",
    "    inds = vct.argsort()\n",
    "    mtx_sorted = mtx.copy()\n",
    "    mtx_sorted = mtx_sorted[inds, :]\n",
    "    mtx_sorted = mtx_sorted[:, inds]    \n",
    "    return mtx_sorted\n",
    "\n",
    "def norm_mtx(mtx):\n",
    "    \"\"\"\n",
    "    Scales a matrix to have values between 0 and 1.\n",
    "    \"\"\"\n",
    "#     return mtx/abs(mtx).max()\n",
    "    return (mtx-np.min(mtx))/(np.max(mtx)-np.min(mtx))\n",
    "\n",
    "\n",
    "# set up subjects info\n",
    "exclude_subs = [1,6,14,18,21,31]\n",
    "n_subs = 46 - len(exclude_subs)\n",
    "\n",
    "# formatting to print axes of sub-by-sub matrix as 's-1, s-2....X,... s-40'\n",
    "sub_ls = ['.']*40\n",
    "sub_ls[0] = 's-1'\n",
    "sub_ls[1] = sub_ls[3] = ''\n",
    "sub_ls[2] = 's-2'\n",
    "sub_ls[4] = 's-3'\n",
    "sub_ls[39] = 's-40'\n",
    "sub_y_ls = copy.deepcopy(sub_ls)\n",
    "sub_ls[16] = 'X'\n",
    "sub_y_ls[22] = 'Y'\n",
    "\n",
    "\n",
    "# set up mask parcels info\n",
    "mask = Brain_Data('http://neurovault.org/media/images/2099/Neurosynth%20Parcellation_0.nii.gz')\n",
    "masker = NiftiLabelsMasker(labels_img=mask.to_nifti(), standardize=True)\n",
    "\n",
    "\n",
    "def set_self_rois():\n",
    "    \n",
    "    global roi_ntwrk, roi_i, roi_names, n_rois\n",
    "    \n",
    "    roi_ntwrk = 'pc-vmpfc'\n",
    "    roi_i = [6,32]\n",
    "    roi_names = ['pc', 'vmpfc']\n",
    "    n_rois = len(roi_i)\n",
    "\n",
    "\n",
    "def set_dmn_rois():\n",
    "    \n",
    "    global roi_ntwrk, roi_i, roi_names, n_rois\n",
    "    \n",
    "    roi_ntwrk = 'DMN'\n",
    "    # indices of relevant rois\n",
    "    roi_i = [2,5,6,19,32,49]\n",
    "    roi_names = ['dmpfc', 'tpj', 'pc', 'pcc', 'vmpfc', 'sts']\n",
    "    n_rois = len(roi_i)\n",
    "    \n",
    "\n",
    "def set_dmn_hippo_rois():\n",
    "    \n",
    "    global roi_ntwrk, roi_i, roi_names, n_rois\n",
    "    \n",
    "    roi_ntwrk = 'DMN_hippo'\n",
    "    # indices of relevant rois\n",
    "    roi_i = [2,5,6,19,28,32,49]\n",
    "    roi_names = ['dmpfc', 'tpj', 'pc', 'pcc', 'hippo', 'vmpfc', 'sts']\n",
    "    n_rois = len(roi_i)\n",
    "    \n",
    "    \n",
    "def set_limbic_rois():\n",
    "    \n",
    "    global roi_ntwrk, roi_i, roi_names, n_rois\n",
    "\n",
    "    roi_ntwrk = 'limbic'\n",
    "    # indices of relevant rois\n",
    "    roi_i = [12,18,22]\n",
    "    roi_names = ['amyg', 'ai', 'dacc']\n",
    "    n_rois = len(roi_i)\n",
    "\n",
    "    \n",
    "def set_limbic_nacc_rois():\n",
    "    \n",
    "    global roi_ntwrk, roi_i, roi_names, n_rois\n",
    "\n",
    "    roi_ntwrk = 'limbic_nacc'\n",
    "    # indices of relevant rois\n",
    "    roi_i = [12,18,22,34]\n",
    "    roi_names = ['amyg', 'ai', 'dacc', 'nacc']\n",
    "    n_rois = len(roi_i)\n",
    "    \n",
    "    \n",
    "def set_whole_brain_rois():\n",
    "\n",
    "    global roi_ntwrk, roi_i, roi_names, n_rois\n",
    "\n",
    "    roi_ntwrk = 'whole-brain'\n",
    "    roi_i = list(range(0,50))\n",
    "    roi_names = pd.read_csv(f'{path}/other_data/neurosynth_chang_parcellation_labels.csv')['label']\n",
    "    n_rois = len(roi_i)\n",
    "\n",
    "    # some formatting to name to make roi names cleaner\n",
    "    for i in range(n_rois):\n",
    "        roi_names[i] = roi_names[i].replace(\" \", \"_\")\n",
    "        roi_names[i] = roi_names[i].replace(\"/\", \"_OR_\")\n",
    "\n",
    "# store rois/parcels\n",
    "parcels = pd.read_csv('./neurosynth_chang_parcellation_labels.csv')['label']\n",
    "\n",
    "# some formatting to name to make roi names cleaner\n",
    "for i in range(len(parcels)):\n",
    "    parcels[i] = parcels[i].replace(\" \", \"_\")\n",
    "    parcels[i] = parcels[i].replace(\"/\", \"_OR_\")\n",
    "    \n",
    "temporal_analysis = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e73221",
   "metadata": {},
   "source": [
    "# 2. Creating subject-pair 1 - mean sentiment matrix (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e6c7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def sentiment_isc():\n",
    "    \"\"\"\n",
    "    makes & displays sentimentioral annak matrix\n",
    "    \"\"\"\n",
    "\n",
    "    sentiment_df = pd.read_csv('./soc_sent_all_txt.csv')    \n",
    "\n",
    "# exclude subs\n",
    "    sentiment_df = sentiment_df[~sentiment_df['subject'].isin(exclude_subs)].reset_index(drop=True)\n",
    "\n",
    "# only select specific 'score' column\n",
    "    sentiment_scores = sentiment_df['sentiment_score'].to_numpy()\n",
    "# convert scores to ranks for spearman correlations\n",
    "    sentiment_rank = rankdata(sentiment_scores)-1\n",
    "\n",
    "# append ranks to df\n",
    "    sentiment_df['score_rank'] = pd.DataFrame(sentiment_rank)\n",
    "\n",
    "\n",
    "# create sentiment mtx\n",
    "    sentiment_mtx = np.zeros((n_subs, n_subs))\n",
    "    for i in range(n_subs):\n",
    "        for j in range(n_subs):\n",
    "            if i!=j:\n",
    "                sim_ij = - 1/n_subs * np.mean([sentiment_rank[i], sentiment_rank[j]])\n",
    "                sentiment_mtx[i,j] = sentiment_mtx[j,i] =  sim_ij\n",
    "\n",
    "    adj_sentiment_mtx = Adjacency(sentiment_mtx, matrix_type='similarity')\n",
    "\n",
    "# plot similarity mtx\n",
    "    fig = plt.figure(figsize=(30,25))\n",
    "    \n",
    "    ax = sns.heatmap(sort_square_mtx(norm_mtx(sentiment_mtx), sentiment_scores), square=True,\\\n",
    "                     xticklabels=sub_ls, yticklabels=sub_y_ls, mask=np.triu(np.ones_like(sentiment_mtx)),\\\n",
    "                     cmap='YlOrRd', vmin=0, vmax=1, cbar=False)\n",
    "\n",
    "    plt.xlabel('subjects sorted,\\nnegative to positive memory affect', fontname='Arial', fontsize=35)\n",
    "    \n",
    "    # add a grey border to each matrix cell\n",
    "    for i in range(n_subs):\n",
    "        for j in range(n_subs):\n",
    "            if j>i:\n",
    "                ax.add_patch(Rectangle((i, j), 1, 1, fill=False, edgecolor='grey', lw=.1))\n",
    "\n",
    "    ax.set_facecolor(\"white\")\n",
    "    plt.xticks(fontsize=30, rotation=45, fontname='Arial')\n",
    "    plt.yticks(fontsize=30, rotation=45, fontname='Arial')\n",
    "    plt.savefig('./figs/model.png', facecolor='white', edgecolor='none')\n",
    "    \n",
    "    return sentiment_mtx, adj_sentiment_mtx, sentiment_df, sentiment_scores\n",
    "\n",
    "# behavioral isc\n",
    "\n",
    "print(f'subs = {n_subs}')\n",
    "sentiment_mtx, adj_sentiment_mtx, sentiment_df, sentiment_scores = sentiment_isc()\n",
    "subs_sorted = sentiment_df.sort_values('score_rank').index.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2663f10f",
   "metadata": {},
   "source": [
    "# 3. Creating subject-pair connectivity similarity matrix (data) and performing ISRSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f127303b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def fc():\n",
    "\n",
    "    global n_subs, phase, roi_ntwrk, roi_i, roi_names, n_rois, start_TR, stop_TR, temporal_analysis\n",
    "\n",
    "# load neural time series & plot settings \n",
    "    ts = np.load(f'./brain_data/{phase}_nodewise_timeseries.npy')\n",
    "    \n",
    "# chopping off first 10s and selecting desired rois        \n",
    "    ts = ts[:, 10: , roi_i]\n",
    "    total_TR_len = len(ts[2])\n",
    "\n",
    "# since we dont run temporal analyses on any run besides 'soc_consol',\n",
    "# the start & end TR (and hence the time window of analysis) is always the entire time series \n",
    "    if not phase == 'soc_consol':\n",
    "        start_TR = 0\n",
    "        stop_TR = total_TR_len\n",
    "    \n",
    "    \n",
    "# subsetting timeseries window\n",
    "    ts = ts[:, start_TR : stop_TR, :]\n",
    "    n_subs, n_tr, n_rois = np.shape(ts)\n",
    "\n",
    "    \n",
    "    # list of subjects' connectivity matrices\n",
    "    sub_mtx_fc = [] # fc mtx\n",
    "\n",
    "    for sub in range(n_subs):\n",
    "        \n",
    "        # create connectivity matrix for each subject\n",
    "        roi_corr = 1 - pairwise_distances(ts[sub,:,np.arange(n_rois)], metric='correlation')\n",
    "        \n",
    "        # append matrix to list of subjects' matrices\n",
    "        sub_mtx_fc.append(Adjacency(roi_corr, matrix_type='similarity'))\n",
    "        \n",
    "# generate subj-by-subj connectivity similarity matrix\n",
    "    neural_mtx = np.ones((n_subs,n_subs))\n",
    "\n",
    "    for i in range(n_subs):\n",
    "        for j in range(n_subs):\n",
    "\n",
    "            if i!=j:\n",
    "\n",
    "    # euclidean similarity of subjects' matrices\n",
    "                    neural_mtx[i,j] = -np.linalg.norm(stats.zscore(sub_mtx_fc[i].squareform()[np.triu_indices(n_rois, k = 1)]) -\\\n",
    "                                                      stats.zscore(sub_mtx_fc[j].squareform()[np.triu_indices(n_rois, k = 1)]))\n",
    "\n",
    "    # convert matrix from type ndarray to Adjacency\n",
    "    adj_neural_mtx = Adjacency(neural_mtx, matrix_type='similarity')\n",
    "\n",
    "    # comute connectivity similarity\n",
    "    fc_isc = adj_neural_mtx.isc(metric='median', n_bootstraps=4000, n_jobs=-1)    \n",
    "    \n",
    "    # compute rsa between model and data (isrsa)\n",
    "    fc_isrsa = adj_neural_mtx.similarity(adj_sentiment_mtx, metric='spearman', n_permute=4000, n_jobs=-1)\n",
    "    isrsa_r = np.round(fc_isrsa['correlation'],3)\n",
    "    isrsa_p = np.round(fc_isrsa['p'],3)              \n",
    "     \n",
    "    if not temporal_analysis:\n",
    "        # plot isrsa\n",
    "        fig = plt.figure(figsize=(30,25))\n",
    "        ax = sns.heatmap(sort_square_mtx(norm_mtx(neural_mtx), sentiment_scores), square=True,\\\n",
    "                    xticklabels=sub_ls, yticklabels=sub_y_ls, mask=np.triu(np.ones_like(neural_mtx)),\\\n",
    "                    cmap='YlOrRd', cbar=False)\n",
    "\n",
    "        ax.set_facecolor(\"white\")\n",
    "        plt.title(f'Anna Karenina Model Fit for\\nDefault Network Connectivity\\n{isrsa_r}, p: {isrsa_p}\\n', fontsize=35, fontname='Arial')\n",
    "        plt.xlabel('subjects sorted,\\nnegative to positive memory affect', fontsize=35)\n",
    "        plt.xticks(rotation=45, fontsize=30)\n",
    "        plt.yticks(rotation=45, fontsize=30)\n",
    "        plt.savefig(f'./figs/{phase}.png', facecolor='white', edgecolor='none')\n",
    "\n",
    "    return isrsa_r, isrsa_p, sub_mtx_fc, adj_neural_mtx\n",
    "\n",
    "\n",
    "# set neural network\n",
    "# set_whole_brain_rois()\n",
    "# set_limbic_nacc_rois()\n",
    "# set_limbic_rois()\n",
    "set_dmn_rois()\n",
    "   \n",
    "temporal_analysis = False\n",
    "\n",
    "# 1\n",
    "phase = 'soc_consol' # post-encoding rest\n",
    "fc()\n",
    "\n",
    "# 2\n",
    "phase = 'soc_enc' # video-watching/encoding\n",
    "fc()\n",
    "\n",
    "# 3\n",
    "phase = 'baseline'\n",
    "fc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2e04a6",
   "metadata": {},
   "source": [
    "# Temporal Immediacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbacb99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_analysis = True\n",
    "phase = 'soc_consol'\n",
    "isrsa_stats = pd.DataFrame(columns=['r', 'p', 'mean_tr', 'tr_window'])\n",
    "\n",
    "# window_len = the no. of TRs over which we perform analysis\n",
    "for window_len in [180]:\n",
    "\n",
    "    # sliding our window each time by 35 TRs\n",
    "    for start_TR in range(0,180,1):\n",
    "\n",
    "        print(start_TR, end='')\n",
    "        stop_TR = start_TR + window_len\n",
    "        # if the last TR of the window exceeds time series length\n",
    "        if stop_TR > 350:\n",
    "            break\n",
    "        \n",
    "        r, p, sub_mtx_fc, adj_neural_mtx = fc()\n",
    "        isrsa_stats.loc[len(isrsa_stats.index)] = [r, p, .5 * (start_TR + stop_TR), f'TR {start_TR}-{stop_TR}']\n",
    "        \n",
    "fig = plt.figure(figsize=(10,7))\n",
    "plt.scatter(isrsa_stats['mean_tr'], isrsa_stats['r'], [5]*len(isrsa_stats))\n",
    "plt.title('Temporal immediacy of\\nAffect Consolidation by the\\nDefault Network\\n', fontsize=35)\n",
    "plt.xlabel('TR of social-consolidation scan', fontsize=35, fontname='Arial')\n",
    "plt.ylabel('Anna Karenina model fit\\n(r value)', fontsize=35, fontname='Arial')\n",
    "plt.xticks(range(0,350,30), rotation = 45, fontsize = 25, fontname='Arial')\n",
    "plt.yticks(fontsize = 25)\n",
    "\n",
    "# r_trhesh = y intercept where p = .05\n",
    "r_thresh=0\n",
    "for i in range(len(isrsa_stats)):\n",
    "    \n",
    "    if isrsa_stats['p'].iloc[i] < .05:        \n",
    "        r_thresh = isrsa_stats['r'].iloc[i] \n",
    "\n",
    "plt.axhline(y = r_thresh, color = 'red', label='p < .05')\n",
    "plt.legend(loc='upper right', prop={'size': 25})\n",
    "plt.show()\n",
    "fig.savefig('./figs/temporal_isrsa.png', facecolor=\"white\", edgecolor='none')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222c21e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "annak_r = {}\n",
    "# list of color values for different ROIs\n",
    "tst = [-100,-75,-50,50,75,100]\n",
    "i = 0 # index of color list\n",
    "set_dmn_rois()\n",
    "\n",
    "for node in range(len(expand_mask(mask))):\n",
    "    if node not in [2,5,6,19,32,49]:\n",
    "        annak_r[node] = 0\n",
    "    else: # only give values to dmn ROIs\n",
    "        annak_r[node] = tst[i]\n",
    "        i = i + 1\n",
    "\n",
    "annak_r_brain = roi_to_brain(pd.Series(annak_r), expand_mask(mask))\n",
    "cmap = colors.LinearSegmentedColormap.from_list(\"\", ['red','orange','yellow','white','green','cyan','blue'])\n",
    "plotting.view_img_on_surf(annak_r_brain.to_nifti(), cmap=cmap, black_bg=True)#.save_as_html(f'{figs_path}/brains/brain.html')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
